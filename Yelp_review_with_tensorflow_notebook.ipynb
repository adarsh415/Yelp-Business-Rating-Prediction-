{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import time\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(filename):\n",
    "    \n",
    "    labels=[]\n",
    "    feature=[]\n",
    "    with open(filename,'r') as f:\n",
    "        line = csv.reader(f)\n",
    "        next(line,None)\n",
    "        for row in line:\n",
    "            feature.append([ np.int32(dictionary[x]) if x in dictionary else 0 for x in row[1].split()])\n",
    "            labels.append(np.int32(row[0]))\n",
    "    return feature, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(fileName):\n",
    "    text=[]\n",
    "    with open(fileName,'r') as f:\n",
    "        line = csv.reader(f)\n",
    "        next(line,None)\n",
    "        for row in line:\n",
    "            text.extend(row[1].replace('\\n','<eos>').split())\n",
    "    count=[['UNK',1]]\n",
    "    count.extend(Counter(text).most_common())\n",
    "    dictionary = dict()\n",
    "    for word ,_ in count:\n",
    "        dictionary[word]=len(dictionary)\n",
    "    reverse_dictionary=dict(zip(dictionary.values(),dictionary.keys()))\n",
    "    del text\n",
    "    return dictionary,reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padd_row(arr,test=False):\n",
    "    if test:\n",
    "        max_len=424\n",
    "    else:\n",
    "        max_len=max(len(row) for row in arr)\n",
    "    arr_padded = np.array([row + [0]*(max_len - len(row)) for row in arr])\n",
    "    return arr_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary,reverse_dictionary=build_vocab('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15694"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15694"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reverse_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature,labels=create_data('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2724"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature=np.array(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_padd = padd_row(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(feature_padd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_data = tf.convert_to_tensor(feature_padd,dtype=tf.int32,name='feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(feature_padd)\n",
    "#x_data[:100,0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(feature,label,batch_size,test=False):\n",
    "    #assert feature.shape[0]%batch_size==0,\"Use batch size in multiple of 10\"\n",
    "    feature_padd = padd_row(feature,test)\n",
    "    #x_data = tf.convert_to_tensor(feature_padd,dtype=tf.int32,name='feature')\n",
    "    #data_size = tf.size(x_data)\n",
    "    no_batch = len(feature_padd)//batch_size\n",
    "    #data = tf.reshape(x_data[:no_batch*batch_size],[batch_size,no_batch]) # or we can user [batch_size,-1]\n",
    "    #labels = tf.convert_to_tensor(np.array(label),dtype=tf.int32,name='label')\n",
    "    label=np.array(label)\n",
    "    x_=feature_padd[:no_batch*batch_size]\n",
    "    y_=label[:no_batch*batch_size]\n",
    "    \n",
    "    for n in range(0, len(x_), batch_size):\n",
    "        #x = data[:,n:n+seq_n]\n",
    "        x = x_[n:n+batch_size]\n",
    "        y = y_[n:n+batch_size]\n",
    "    \n",
    "        yield x ,y   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = create_batch(feature,labels,50,424)\n",
    "#x,y=next(batch)\n",
    "#print (x.shape)\n",
    "#print (y)\n",
    "#for e in range(20):\n",
    "    #for x,y in create_batch(feature,labels,100):\n",
    "        #print (x)\n",
    "        #print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cell(lstm_size,keep_prob):\n",
    "    with tf.variable_scope(\"cells\"):\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(lstm_size)\n",
    "        drop = tf.nn.rnn_cell.DropoutWrapper(cell,output_keep_prob=keep_prob)\n",
    "    return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input(batch_size,num_setps,num_classes):\n",
    "    inputs = tf.placeholder(tf.int32,[batch_size,num_setps],name='input')\n",
    "    targets = tf.placeholder(tf.int32,[batch_size],name='targets')\n",
    "    keep_proba = tf.placeholder(tf.float32,name='keep_proba')\n",
    "    \n",
    "    return inputs,targets,keep_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size,num_layers,batch_size,keep_proba):\n",
    "    with tf.name_scope(\"LSTM_Network\"):\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([create_cell(lstm_size,keep_proba) for _ in range(num_layers)])\n",
    "        initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "    \n",
    "    return cell,initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpNetwork(object):\n",
    "    def __init__(self,\n",
    "                 lstm_size,\n",
    "                 batch_size,\n",
    "                 num_layers,\n",
    "                 vocab,\n",
    "                 numClasses=2,\n",
    "                 num_steps=424,\n",
    "                 learning_rate=0.001,\n",
    "                 grad_clip=0.5,\n",
    "                 embedding_size=300):\n",
    "        \n",
    "        self.lstm_size=lstm_size\n",
    "        self.batch_size=batch_size\n",
    "        self.num_layers=num_layers\n",
    "        self.vocab_size=len(vocab)\n",
    "        self.numClasses=numClasses\n",
    "        self.grad_clip=grad_clip\n",
    "        self.inputs,self.target,self.keep_proba=build_input(self.batch_size,num_steps,numClasses)\n",
    "        self.learning_rate=learning_rate\n",
    "        self.embedding= self.embedding_matrix(self.inputs,embedding_size,self.vocab_size)\n",
    "        self.logits,self.out=self.build_output()\n",
    "        self.loss=self.build_loss()\n",
    "        self.optimizer=self.build_optimizer()\n",
    "        self.accuracy =self.accuracy()\n",
    "        \n",
    "    \n",
    "    def embedding_matrix(self,x,embedding_size,vocab_size):\n",
    "        with tf.variable_scope('embedding'):\n",
    "            embedd = tf.Variable(tf.random_uniform([vocab_size,embedding_size],-0.05,0.05))\n",
    "            embedding=tf.nn.embedding_lookup(embedd,x)\n",
    "        return embedding\n",
    "    \n",
    "    def build_output(self):\n",
    "        cell,self.initial_state = build_lstm(self.lstm_size,self.num_layers,self.batch_size,self.keep_proba)\n",
    "        output,state = tf.nn.dynamic_rnn(cell, self.embedding,initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        #output_flat = tf.reshape(output,[:,-1])\n",
    "        \n",
    "        with tf.variable_scope('softmax',reuse=True):\n",
    "            softmax_w = tf.Variable(tf.truncated_normal([self.lstm_size,self.numClasses],stddev=0.1))\n",
    "            softmax_b = tf.Variable(tf.truncated_normal([self.numClasses],stddev=0.1))\n",
    "        \n",
    "        logits = tf.nn.xw_plus_b(output[:,-1],softmax_w,softmax_b)\n",
    "        #logits = tf.sigmoid(logits,name='sigmoid')\n",
    "        out = tf.nn.sigmoid(logits,name='predictions')\n",
    "        #out = tf.argmax(out,axis=1)\n",
    "        return logits,out\n",
    "    \n",
    "    def build_loss(self):\n",
    "        \n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            \n",
    "            y_one_hot = tf.one_hot(self.target,self.numClasses)\n",
    "            #y_reshaped = tf.reshape(y_one_hot,(self.logits.get_shape()))\n",
    "        \n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_one_hot,logits=self.logits)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def build_optimizer(self):\n",
    "        \n",
    "        with tf.variable_scope(\"optimizer\"):\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads,_ = tf.clip_by_global_norm(tf.gradients(self.loss,tvars),self.grad_clip)\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            optimizer = train_op.apply_gradients(zip(grads,tvars))\n",
    "        return optimizer\n",
    "   \n",
    "    def accuracy(self):\n",
    "        with tf.variable_scope(\"accuracy\"):\n",
    "            y_one_hot_test = tf.one_hot(self.target,self.numClasses)\n",
    "            correct_predictions = tf.equal(tf.argmax(self.out,1),tf.argmax(y_one_hot_test,1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_predictions,'float'))\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100         # Sequences per batch\n",
    "#num_steps = 100          # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "#learning_rate = 0.001    # Learning rate\n",
    "keep_prob = 0.5   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-21-c57825aac3f0>:57: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Epoch: 1/20...  Training Step: 1...  Training loss: 0.1450...  21.6550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2...  Training loss: 0.1012...  27.7580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 3...  Training loss: 0.0374...  25.4710 sec/batch\n",
      "Epoch: 1/20...  Training Step: 4...  Training loss: 0.1000...  23.0020 sec/batch\n",
      "Epoch: 1/20...  Training Step: 5...  Training loss: 0.2003...  23.5980 sec/batch\n",
      "Epoch: 1/20...  Training Step: 6...  Training loss: 0.0987...  23.4050 sec/batch\n",
      "Epoch: 1/20...  Training Step: 7...  Training loss: 0.0950...  22.4700 sec/batch\n",
      "Epoch: 1/20...  Training Step: 8...  Training loss: 0.1085...  22.0500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 9...  Training loss: 0.1231...  21.8230 sec/batch\n",
      "Epoch: 1/20...  Training Step: 10...  Training loss: 0.1924...  23.7130 sec/batch\n",
      "Epoch: 1/20...  Training Step: 11...  Training loss: 0.2556...  24.2110 sec/batch\n",
      "Epoch: 1/20...  Training Step: 12...  Training loss: 0.2205...  22.2730 sec/batch\n",
      "Epoch: 1/20...  Training Step: 13...  Training loss: 0.1174...  22.2930 sec/batch\n",
      "Epoch: 1/20...  Training Step: 14...  Training loss: 0.0983...  22.2260 sec/batch\n",
      "Epoch: 1/20...  Training Step: 15...  Training loss: 0.1096...  22.3570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 16...  Training loss: 0.1046...  22.4740 sec/batch\n",
      "Epoch: 1/20...  Training Step: 17...  Training loss: 0.0718...  23.5550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 18...  Training loss: 0.1647...  22.8500 sec/batch\n",
      "Epoch: 1/20...  Training Step: 19...  Training loss: 0.0721...  24.1620 sec/batch\n",
      "Epoch: 1/20...  Training Step: 20...  Training loss: 0.1790...  22.5160 sec/batch\n",
      "Epoch: 1/20...  Training Step: 21...  Training loss: 0.0611...  24.9090 sec/batch\n",
      "Epoch: 1/20...  Training Step: 22...  Training loss: 0.1928...  23.4750 sec/batch\n",
      "Epoch: 1/20...  Training Step: 23...  Training loss: 0.1304...  25.6180 sec/batch\n",
      "Epoch: 1/20...  Training Step: 24...  Training loss: 0.2004...  24.6460 sec/batch\n",
      "Epoch: 1/20...  Training Step: 25...  Training loss: 0.2704...  22.9520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 26...  Training loss: 0.1662...  23.2580 sec/batch\n",
      "Epoch: 1/20...  Training Step: 27...  Training loss: 0.1739...  24.3540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 28...  Training loss: 0.2062...  23.0570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 29...  Training loss: 0.1291...  23.2990 sec/batch\n",
      "Epoch: 2/20...  Training Step: 30...  Training loss: 0.2397...  23.3150 sec/batch\n",
      "Epoch: 2/20...  Training Step: 31...  Training loss: 0.1739...  25.3770 sec/batch\n",
      "Epoch: 2/20...  Training Step: 32...  Training loss: 0.0978...  23.7830 sec/batch\n",
      "Epoch: 2/20...  Training Step: 33...  Training loss: 0.1958...  23.4060 sec/batch\n",
      "Epoch: 2/20...  Training Step: 34...  Training loss: 0.1597...  23.6350 sec/batch\n",
      "Epoch: 2/20...  Training Step: 35...  Training loss: 0.1752...  24.7820 sec/batch\n",
      "Epoch: 2/20...  Training Step: 36...  Training loss: 0.1702...  26.4060 sec/batch\n",
      "Epoch: 2/20...  Training Step: 37...  Training loss: 0.0639...  24.8110 sec/batch\n",
      "Epoch: 2/20...  Training Step: 38...  Training loss: 0.2065...  24.8280 sec/batch\n",
      "Epoch: 2/20...  Training Step: 39...  Training loss: 0.2167...  23.9350 sec/batch\n",
      "Epoch: 2/20...  Training Step: 40...  Training loss: 0.1027...  23.8040 sec/batch\n",
      "Epoch: 2/20...  Training Step: 41...  Training loss: 0.1149...  26.1890 sec/batch\n",
      "Epoch: 2/20...  Training Step: 42...  Training loss: 0.1557...  23.9020 sec/batch\n",
      "Epoch: 2/20...  Training Step: 43...  Training loss: 0.0883...  23.8810 sec/batch\n",
      "Epoch: 2/20...  Training Step: 44...  Training loss: 0.1502...  24.1760 sec/batch\n",
      "Epoch: 2/20...  Training Step: 45...  Training loss: 0.1624...  23.9920 sec/batch\n",
      "Epoch: 2/20...  Training Step: 46...  Training loss: 0.0776...  24.4600 sec/batch\n",
      "Epoch: 2/20...  Training Step: 47...  Training loss: 0.1804...  25.9080 sec/batch\n",
      "Epoch: 2/20...  Training Step: 48...  Training loss: 0.1051...  27.2830 sec/batch\n",
      "Epoch: 2/20...  Training Step: 49...  Training loss: 0.2148...  24.3910 sec/batch\n",
      "Epoch: 2/20...  Training Step: 50...  Training loss: 0.1258...  24.3440 sec/batch\n",
      "Epoch: 2/20...  Training Step: 51...  Training loss: 0.1560...  26.6080 sec/batch\n",
      "Epoch: 2/20...  Training Step: 52...  Training loss: 0.1032...  24.3570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 53...  Training loss: 0.1273...  24.3770 sec/batch\n",
      "Epoch: 2/20...  Training Step: 54...  Training loss: 0.1631...  25.5370 sec/batch\n",
      "Epoch: 3/20...  Training Step: 55...  Training loss: 0.2205...  24.7680 sec/batch\n",
      "Epoch: 3/20...  Training Step: 56...  Training loss: 0.2019...  26.0470 sec/batch\n",
      "Epoch: 3/20...  Training Step: 57...  Training loss: 0.1230...  25.6700 sec/batch\n",
      "Epoch: 3/20...  Training Step: 58...  Training loss: 0.1593...  24.7070 sec/batch\n",
      "Epoch: 3/20...  Training Step: 59...  Training loss: 0.1396...  26.9420 sec/batch\n",
      "Epoch: 3/20...  Training Step: 60...  Training loss: 0.2377...  27.0180 sec/batch\n",
      "Epoch: 3/20...  Training Step: 61...  Training loss: 0.1850...  27.4210 sec/batch\n",
      "Epoch: 3/20...  Training Step: 62...  Training loss: 0.1052...  26.0760 sec/batch\n",
      "Epoch: 3/20...  Training Step: 63...  Training loss: 0.2032...  25.0040 sec/batch\n",
      "Epoch: 3/20...  Training Step: 64...  Training loss: 0.1331...  26.6850 sec/batch\n",
      "Epoch: 3/20...  Training Step: 65...  Training loss: 0.3272...  26.8300 sec/batch\n",
      "Epoch: 3/20...  Training Step: 66...  Training loss: 0.2537...  25.2030 sec/batch\n",
      "Epoch: 3/20...  Training Step: 67...  Training loss: 0.1679...  25.8480 sec/batch\n",
      "Epoch: 3/20...  Training Step: 68...  Training loss: 0.1579...  25.5280 sec/batch\n",
      "Epoch: 3/20...  Training Step: 69...  Training loss: 0.2149...  25.1110 sec/batch\n",
      "Epoch: 3/20...  Training Step: 70...  Training loss: 0.0706...  27.0840 sec/batch\n",
      "Epoch: 3/20...  Training Step: 71...  Training loss: 0.1419...  28.3390 sec/batch\n",
      "Epoch: 3/20...  Training Step: 72...  Training loss: 0.1388...  27.5510 sec/batch\n",
      "Epoch: 3/20...  Training Step: 73...  Training loss: 0.1339...  26.1700 sec/batch\n",
      "Epoch: 3/20...  Training Step: 74...  Training loss: 0.2605...  26.8040 sec/batch\n",
      "Epoch: 3/20...  Training Step: 75...  Training loss: 0.0955...  25.7250 sec/batch\n",
      "Epoch: 3/20...  Training Step: 76...  Training loss: 0.1684...  27.0090 sec/batch\n",
      "Epoch: 3/20...  Training Step: 77...  Training loss: 0.0942...  26.8800 sec/batch\n",
      "Epoch: 3/20...  Training Step: 78...  Training loss: 0.0557...  25.5890 sec/batch\n",
      "Epoch: 3/20...  Training Step: 79...  Training loss: 0.1263...  27.8640 sec/batch\n",
      "Epoch: 3/20...  Training Step: 80...  Training loss: 0.1865...  29.5840 sec/batch\n",
      "Epoch: 3/20...  Training Step: 81...  Training loss: 0.1452...  26.9640 sec/batch\n",
      "Epoch: 4/20...  Training Step: 82...  Training loss: 0.2172...  29.1000 sec/batch\n",
      "Epoch: 4/20...  Training Step: 83...  Training loss: 0.1343...  28.0370 sec/batch\n",
      "Epoch: 4/20...  Training Step: 84...  Training loss: 0.2189...  26.5400 sec/batch\n",
      "Epoch: 4/20...  Training Step: 85...  Training loss: 0.1580...  25.7710 sec/batch\n",
      "Epoch: 4/20...  Training Step: 86...  Training loss: 0.2110...  27.2100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 87...  Training loss: 0.3115...  27.8440 sec/batch\n",
      "Epoch: 4/20...  Training Step: 88...  Training loss: 0.1268...  26.8240 sec/batch\n",
      "Epoch: 4/20...  Training Step: 89...  Training loss: 0.3313...  26.9570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 90...  Training loss: 0.1408...  29.7160 sec/batch\n",
      "Epoch: 4/20...  Training Step: 91...  Training loss: 0.2120...  27.5460 sec/batch\n",
      "Epoch: 4/20...  Training Step: 92...  Training loss: 0.2959...  28.0610 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 93...  Training loss: 0.1869...  29.0770 sec/batch\n",
      "Epoch: 4/20...  Training Step: 94...  Training loss: 0.1270...  26.2300 sec/batch\n",
      "Epoch: 4/20...  Training Step: 95...  Training loss: 0.1437...  28.4420 sec/batch\n",
      "Epoch: 4/20...  Training Step: 96...  Training loss: 0.2196...  28.0050 sec/batch\n",
      "Epoch: 4/20...  Training Step: 97...  Training loss: 0.1003...  26.3370 sec/batch\n",
      "Epoch: 4/20...  Training Step: 98...  Training loss: 0.0727...  26.1820 sec/batch\n",
      "Epoch: 4/20...  Training Step: 99...  Training loss: 0.2920...  27.7360 sec/batch\n",
      "Epoch: 4/20...  Training Step: 100...  Training loss: 0.1503...  28.2270 sec/batch\n",
      "Epoch: 4/20...  Training Step: 101...  Training loss: 0.1807...  26.8870 sec/batch\n",
      "Epoch: 4/20...  Training Step: 102...  Training loss: 0.1056...  27.1570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 103...  Training loss: 0.1909...  27.1230 sec/batch\n",
      "Epoch: 4/20...  Training Step: 104...  Training loss: 0.1045...  28.8870 sec/batch\n",
      "Epoch: 4/20...  Training Step: 105...  Training loss: 0.2590...  28.4660 sec/batch\n",
      "Epoch: 4/20...  Training Step: 106...  Training loss: 0.1993...  26.3410 sec/batch\n",
      "Epoch: 4/20...  Training Step: 107...  Training loss: 0.1245...  26.5190 sec/batch\n",
      "Epoch: 4/20...  Training Step: 108...  Training loss: 0.0756...  27.6700 sec/batch\n",
      "Epoch: 5/20...  Training Step: 109...  Training loss: 0.2744...  27.5970 sec/batch\n",
      "Epoch: 5/20...  Training Step: 110...  Training loss: 0.2460...  26.5200 sec/batch\n",
      "Epoch: 5/20...  Training Step: 111...  Training loss: 0.1475...  28.2300 sec/batch\n",
      "Epoch: 5/20...  Training Step: 112...  Training loss: 0.1262...  27.0300 sec/batch\n",
      "Epoch: 5/20...  Training Step: 113...  Training loss: 0.1408...  26.8000 sec/batch\n",
      "Epoch: 5/20...  Training Step: 114...  Training loss: 0.1721...  28.7500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 115...  Training loss: 0.1371...  29.0710 sec/batch\n",
      "Epoch: 5/20...  Training Step: 116...  Training loss: 0.1528...  26.5430 sec/batch\n",
      "Epoch: 5/20...  Training Step: 117...  Training loss: 0.0902...  26.7270 sec/batch\n",
      "Epoch: 5/20...  Training Step: 118...  Training loss: 0.1294...  27.8970 sec/batch\n",
      "Epoch: 5/20...  Training Step: 119...  Training loss: 0.2110...  27.9870 sec/batch\n",
      "Epoch: 5/20...  Training Step: 120...  Training loss: 0.1278...  26.6730 sec/batch\n",
      "Epoch: 5/20...  Training Step: 121...  Training loss: 0.1970...  29.2390 sec/batch\n",
      "Epoch: 5/20...  Training Step: 122...  Training loss: 0.1656...  26.6590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 123...  Training loss: 0.2876...  28.0290 sec/batch\n",
      "Epoch: 5/20...  Training Step: 124...  Training loss: 0.1167...  27.4790 sec/batch\n",
      "Epoch: 5/20...  Training Step: 125...  Training loss: 0.1342...  30.1050 sec/batch\n",
      "Epoch: 5/20...  Training Step: 126...  Training loss: 0.2457...  30.0350 sec/batch\n",
      "Epoch: 5/20...  Training Step: 127...  Training loss: 0.1291...  27.9170 sec/batch\n",
      "Epoch: 5/20...  Training Step: 128...  Training loss: 0.0978...  27.1070 sec/batch\n",
      "Epoch: 5/20...  Training Step: 129...  Training loss: 0.1330...  26.9800 sec/batch\n",
      "Epoch: 5/20...  Training Step: 130...  Training loss: 0.1269...  27.7160 sec/batch\n",
      "Epoch: 5/20...  Training Step: 131...  Training loss: 0.2385...  28.8740 sec/batch\n",
      "Epoch: 5/20...  Training Step: 132...  Training loss: 0.2062...  27.5440 sec/batch\n",
      "Epoch: 5/20...  Training Step: 133...  Training loss: 0.1434...  27.1730 sec/batch\n",
      "Epoch: 5/20...  Training Step: 134...  Training loss: 0.2019...  27.0280 sec/batch\n",
      "Epoch: 5/20...  Training Step: 135...  Training loss: 0.1028...  28.3810 sec/batch\n",
      "Epoch: 6/20...  Training Step: 136...  Training loss: 0.1877...  30.0420 sec/batch\n",
      "Epoch: 6/20...  Training Step: 137...  Training loss: 0.1283...  26.9240 sec/batch\n",
      "Epoch: 6/20...  Training Step: 138...  Training loss: 0.1004...  27.2350 sec/batch\n",
      "Epoch: 6/20...  Training Step: 139...  Training loss: 0.1269...  28.9580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 140...  Training loss: 0.2425...  28.3400 sec/batch\n",
      "Epoch: 6/20...  Training Step: 141...  Training loss: 0.2690...  29.8080 sec/batch\n",
      "Epoch: 6/20...  Training Step: 142...  Training loss: 0.0940...  27.7790 sec/batch\n",
      "Epoch: 6/20...  Training Step: 143...  Training loss: 0.1988...  27.5730 sec/batch\n",
      "Epoch: 6/20...  Training Step: 144...  Training loss: 0.1020...  27.3790 sec/batch\n",
      "Epoch: 6/20...  Training Step: 145...  Training loss: 0.1301...  28.0640 sec/batch\n",
      "Epoch: 6/20...  Training Step: 146...  Training loss: 0.3978...  27.8290 sec/batch\n",
      "Epoch: 6/20...  Training Step: 147...  Training loss: 0.1694...  29.5320 sec/batch\n",
      "Epoch: 6/20...  Training Step: 148...  Training loss: 0.1904...  28.8810 sec/batch\n",
      "Epoch: 6/20...  Training Step: 149...  Training loss: 0.2017...  27.2390 sec/batch\n",
      "Epoch: 6/20...  Training Step: 150...  Training loss: 0.1889...  28.4020 sec/batch\n",
      "Epoch: 6/20...  Training Step: 151...  Training loss: 0.0429...  32.5440 sec/batch\n",
      "Epoch: 6/20...  Training Step: 152...  Training loss: 0.1629...  26.9360 sec/batch\n",
      "Epoch: 6/20...  Training Step: 153...  Training loss: 0.1504...  27.6150 sec/batch\n",
      "Epoch: 6/20...  Training Step: 154...  Training loss: 0.1632...  28.0210 sec/batch\n",
      "Epoch: 6/20...  Training Step: 155...  Training loss: 0.1158...  27.2220 sec/batch\n",
      "Epoch: 6/20...  Training Step: 156...  Training loss: 0.0675...  29.8680 sec/batch\n",
      "Epoch: 6/20...  Training Step: 157...  Training loss: 0.1277...  30.7440 sec/batch\n",
      "Epoch: 6/20...  Training Step: 158...  Training loss: 0.0971...  28.7010 sec/batch\n",
      "Epoch: 6/20...  Training Step: 159...  Training loss: 0.1043...  29.9140 sec/batch\n",
      "Epoch: 6/20...  Training Step: 160...  Training loss: 0.0802...  29.3590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 161...  Training loss: 0.2642...  29.4410 sec/batch\n",
      "Epoch: 6/20...  Training Step: 162...  Training loss: 0.1284...  28.7090 sec/batch\n",
      "Epoch: 7/20...  Training Step: 163...  Training loss: 0.1900...  28.2670 sec/batch\n",
      "Epoch: 7/20...  Training Step: 164...  Training loss: 0.1693...  28.3260 sec/batch\n",
      "Epoch: 7/20...  Training Step: 165...  Training loss: 0.2245...  27.9100 sec/batch\n",
      "Epoch: 7/20...  Training Step: 166...  Training loss: 0.1659...  28.5410 sec/batch\n",
      "Epoch: 7/20...  Training Step: 167...  Training loss: 0.1512...  29.3090 sec/batch\n",
      "Epoch: 7/20...  Training Step: 168...  Training loss: 0.1863...  29.6100 sec/batch\n",
      "Epoch: 7/20...  Training Step: 169...  Training loss: 0.0905...  29.7950 sec/batch\n",
      "Epoch: 7/20...  Training Step: 170...  Training loss: 0.2504...  29.4650 sec/batch\n",
      "Epoch: 7/20...  Training Step: 171...  Training loss: 0.0907...  29.3410 sec/batch\n",
      "Epoch: 7/20...  Training Step: 172...  Training loss: 0.1360...  27.7360 sec/batch\n",
      "Epoch: 7/20...  Training Step: 173...  Training loss: 0.2596...  27.6830 sec/batch\n",
      "Epoch: 7/20...  Training Step: 174...  Training loss: 0.1158...  29.7830 sec/batch\n",
      "Epoch: 7/20...  Training Step: 175...  Training loss: 0.2188...  28.4520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 176...  Training loss: 0.2243...  29.0540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 177...  Training loss: 0.2500...  31.6980 sec/batch\n",
      "Epoch: 7/20...  Training Step: 178...  Training loss: 0.2239...  29.4440 sec/batch\n",
      "Epoch: 7/20...  Training Step: 179...  Training loss: 0.1142...  27.7110 sec/batch\n",
      "Epoch: 7/20...  Training Step: 180...  Training loss: 0.0879...  28.4180 sec/batch\n",
      "Epoch: 7/20...  Training Step: 181...  Training loss: 0.2012...  29.8370 sec/batch\n",
      "Epoch: 7/20...  Training Step: 182...  Training loss: 0.1876...  28.9540 sec/batch\n",
      "Epoch: 7/20...  Training Step: 183...  Training loss: 0.1110...  30.0190 sec/batch\n",
      "Epoch: 7/20...  Training Step: 184...  Training loss: 0.1790...  28.8210 sec/batch\n",
      "Epoch: 7/20...  Training Step: 185...  Training loss: 0.1518...  28.9020 sec/batch\n",
      "Epoch: 7/20...  Training Step: 186...  Training loss: 0.2239...  28.6320 sec/batch\n",
      "Epoch: 7/20...  Training Step: 187...  Training loss: 0.1045...  29.9060 sec/batch\n",
      "Epoch: 7/20...  Training Step: 188...  Training loss: 0.3012...  31.8620 sec/batch\n",
      "Epoch: 7/20...  Training Step: 189...  Training loss: 0.1780...  29.2640 sec/batch\n",
      "Epoch: 8/20...  Training Step: 190...  Training loss: 0.2789...  28.1710 sec/batch\n",
      "Epoch: 8/20...  Training Step: 191...  Training loss: 0.1959...  30.3660 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 192...  Training loss: 0.1377...  28.6500 sec/batch\n",
      "Epoch: 8/20...  Training Step: 193...  Training loss: 0.1691...  28.9450 sec/batch\n",
      "Epoch: 8/20...  Training Step: 194...  Training loss: 0.1256...  29.0190 sec/batch\n",
      "Epoch: 8/20...  Training Step: 195...  Training loss: 0.1437...  30.8370 sec/batch\n",
      "Epoch: 8/20...  Training Step: 196...  Training loss: 0.0897...  27.7590 sec/batch\n",
      "Epoch: 8/20...  Training Step: 197...  Training loss: 0.2266...  29.5720 sec/batch\n",
      "Epoch: 8/20...  Training Step: 198...  Training loss: 0.1009...  30.5310 sec/batch\n",
      "Epoch: 8/20...  Training Step: 199...  Training loss: 0.0802...  28.0370 sec/batch\n",
      "Epoch: 8/20...  Training Step: 200...  Training loss: 0.2574...  28.8110 sec/batch\n",
      "Epoch: 8/20...  Training Step: 201...  Training loss: 0.0966...  30.8090 sec/batch\n",
      "Epoch: 8/20...  Training Step: 202...  Training loss: 0.1521...  27.9950 sec/batch\n",
      "Epoch: 8/20...  Training Step: 203...  Training loss: 0.0880...  30.0180 sec/batch\n",
      "Epoch: 8/20...  Training Step: 204...  Training loss: 0.2933...  27.9610 sec/batch\n",
      "Epoch: 8/20...  Training Step: 205...  Training loss: 0.1474...  29.3420 sec/batch\n",
      "Epoch: 8/20...  Training Step: 206...  Training loss: 0.1121...  29.3960 sec/batch\n",
      "Epoch: 8/20...  Training Step: 207...  Training loss: 0.2309...  28.1470 sec/batch\n",
      "Epoch: 8/20...  Training Step: 208...  Training loss: 0.1932...  30.2960 sec/batch\n",
      "Epoch: 8/20...  Training Step: 209...  Training loss: 0.2178...  31.0020 sec/batch\n",
      "Epoch: 8/20...  Training Step: 210...  Training loss: 0.0861...  31.0990 sec/batch\n",
      "Epoch: 8/20...  Training Step: 211...  Training loss: 0.1288...  30.3140 sec/batch\n",
      "Epoch: 8/20...  Training Step: 212...  Training loss: 0.1195...  29.6030 sec/batch\n",
      "Epoch: 8/20...  Training Step: 213...  Training loss: 0.2250...  29.0340 sec/batch\n",
      "Epoch: 8/20...  Training Step: 214...  Training loss: 0.1457...  30.3450 sec/batch\n",
      "Epoch: 8/20...  Training Step: 215...  Training loss: 0.2216...  29.5030 sec/batch\n",
      "Epoch: 8/20...  Training Step: 216...  Training loss: 0.0467...  28.2560 sec/batch\n",
      "Epoch: 9/20...  Training Step: 217...  Training loss: 0.2240...  29.3130 sec/batch\n",
      "Epoch: 9/20...  Training Step: 218...  Training loss: 0.1870...  31.8700 sec/batch\n",
      "Epoch: 9/20...  Training Step: 219...  Training loss: 0.1499...  30.2090 sec/batch\n",
      "Epoch: 9/20...  Training Step: 220...  Training loss: 0.1808...  30.3220 sec/batch\n",
      "Epoch: 9/20...  Training Step: 221...  Training loss: 0.1556...  29.7780 sec/batch\n",
      "Epoch: 9/20...  Training Step: 222...  Training loss: 0.1631...  29.6710 sec/batch\n",
      "Epoch: 9/20...  Training Step: 223...  Training loss: 0.1671...  29.1680 sec/batch\n",
      "Epoch: 9/20...  Training Step: 224...  Training loss: 0.1496...  28.2240 sec/batch\n",
      "Epoch: 9/20...  Training Step: 225...  Training loss: 0.1793...  29.0710 sec/batch\n",
      "Epoch: 9/20...  Training Step: 226...  Training loss: 0.1524...  32.3280 sec/batch\n",
      "Epoch: 9/20...  Training Step: 227...  Training loss: 0.2521...  29.3440 sec/batch\n",
      "Epoch: 9/20...  Training Step: 228...  Training loss: 0.2641...  30.3740 sec/batch\n",
      "Epoch: 9/20...  Training Step: 229...  Training loss: 0.1225...  30.6100 sec/batch\n",
      "Epoch: 9/20...  Training Step: 230...  Training loss: 0.2352...  28.2710 sec/batch\n",
      "Epoch: 9/20...  Training Step: 231...  Training loss: 0.3318...  29.9460 sec/batch\n",
      "Epoch: 9/20...  Training Step: 232...  Training loss: 0.1741...  29.2170 sec/batch\n",
      "Epoch: 9/20...  Training Step: 233...  Training loss: 0.1762...  30.1530 sec/batch\n",
      "Epoch: 9/20...  Training Step: 234...  Training loss: 0.1310...  29.3260 sec/batch\n",
      "Epoch: 9/20...  Training Step: 235...  Training loss: 0.2061...  29.0880 sec/batch\n",
      "Epoch: 9/20...  Training Step: 236...  Training loss: 0.0389...  29.6680 sec/batch\n",
      "Epoch: 9/20...  Training Step: 237...  Training loss: 0.1026...  29.2070 sec/batch\n",
      "Epoch: 9/20...  Training Step: 238...  Training loss: 0.0853...  30.7120 sec/batch\n",
      "Epoch: 9/20...  Training Step: 239...  Training loss: 0.1283...  32.4660 sec/batch\n",
      "Epoch: 9/20...  Training Step: 240...  Training loss: 0.1742...  29.1900 sec/batch\n",
      "Epoch: 9/20...  Training Step: 241...  Training loss: 0.3200...  31.4800 sec/batch\n",
      "Epoch: 9/20...  Training Step: 242...  Training loss: 0.1654...  28.8420 sec/batch\n",
      "Epoch: 9/20...  Training Step: 243...  Training loss: 0.1309...  29.9130 sec/batch\n",
      "Epoch: 10/20...  Training Step: 244...  Training loss: 0.2001...  28.5960 sec/batch\n",
      "Epoch: 10/20...  Training Step: 245...  Training loss: 0.1689...  32.2380 sec/batch\n",
      "Epoch: 10/20...  Training Step: 246...  Training loss: 0.2868...  30.2540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 247...  Training loss: 0.1160...  29.5820 sec/batch\n",
      "Epoch: 10/20...  Training Step: 248...  Training loss: 0.0729...  32.6380 sec/batch\n",
      "Epoch: 10/20...  Training Step: 249...  Training loss: 0.3248...  32.2440 sec/batch\n",
      "Epoch: 10/20...  Training Step: 250...  Training loss: 0.0949...  30.1900 sec/batch\n",
      "Epoch: 10/20...  Training Step: 251...  Training loss: 0.2347...  30.7580 sec/batch\n",
      "Epoch: 10/20...  Training Step: 252...  Training loss: 0.1144...  28.5470 sec/batch\n",
      "Epoch: 10/20...  Training Step: 253...  Training loss: 0.1138...  30.4440 sec/batch\n",
      "Epoch: 10/20...  Training Step: 254...  Training loss: 0.2714...  32.2280 sec/batch\n",
      "Epoch: 10/20...  Training Step: 255...  Training loss: 0.1062...  28.6540 sec/batch\n",
      "Epoch: 10/20...  Training Step: 256...  Training loss: 0.0987...  28.8310 sec/batch\n",
      "Epoch: 10/20...  Training Step: 257...  Training loss: 0.1666...  30.0870 sec/batch\n",
      "Epoch: 10/20...  Training Step: 258...  Training loss: 0.1564...  31.5330 sec/batch\n",
      "Epoch: 10/20...  Training Step: 259...  Training loss: 0.1062...  29.2280 sec/batch\n",
      "Epoch: 10/20...  Training Step: 260...  Training loss: 0.0654...  28.9190 sec/batch\n",
      "Epoch: 10/20...  Training Step: 261...  Training loss: 0.1817...  32.0790 sec/batch\n",
      "Epoch: 10/20...  Training Step: 262...  Training loss: 0.2044...  29.5110 sec/batch\n",
      "Epoch: 10/20...  Training Step: 263...  Training loss: 0.1544...  29.6410 sec/batch\n",
      "Epoch: 10/20...  Training Step: 264...  Training loss: 0.1224...  28.6570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 265...  Training loss: 0.1394...  29.8290 sec/batch\n",
      "Epoch: 10/20...  Training Step: 266...  Training loss: 0.1182...  28.4230 sec/batch\n",
      "Epoch: 10/20...  Training Step: 267...  Training loss: 0.1101...  31.2840 sec/batch\n",
      "Epoch: 10/20...  Training Step: 268...  Training loss: 0.0975...  31.6140 sec/batch\n",
      "Epoch: 10/20...  Training Step: 269...  Training loss: 0.2727...  29.1650 sec/batch\n",
      "Epoch: 10/20...  Training Step: 270...  Training loss: 0.1474...  28.6660 sec/batch\n",
      "Epoch: 11/20...  Training Step: 271...  Training loss: 0.1912...  30.2130 sec/batch\n",
      "Epoch: 11/20...  Training Step: 272...  Training loss: 0.1756...  31.4720 sec/batch\n",
      "Epoch: 11/20...  Training Step: 273...  Training loss: 0.2524...  29.5930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 274...  Training loss: 0.2644...  32.0660 sec/batch\n",
      "Epoch: 11/20...  Training Step: 275...  Training loss: 0.2098...  28.3960 sec/batch\n",
      "Epoch: 11/20...  Training Step: 276...  Training loss: 0.2632...  29.5290 sec/batch\n",
      "Epoch: 11/20...  Training Step: 277...  Training loss: 0.1208...  29.6630 sec/batch\n",
      "Epoch: 11/20...  Training Step: 278...  Training loss: 0.0973...  32.3310 sec/batch\n",
      "Epoch: 11/20...  Training Step: 279...  Training loss: 0.1639...  30.1170 sec/batch\n",
      "Epoch: 11/20...  Training Step: 280...  Training loss: 0.1772...  28.4380 sec/batch\n",
      "Epoch: 11/20...  Training Step: 281...  Training loss: 0.3199...  31.3240 sec/batch\n",
      "Epoch: 11/20...  Training Step: 282...  Training loss: 0.1141...  30.2710 sec/batch\n",
      "Epoch: 11/20...  Training Step: 283...  Training loss: 0.1053...  28.3370 sec/batch\n",
      "Epoch: 11/20...  Training Step: 284...  Training loss: 0.1307...  31.0130 sec/batch\n",
      "Epoch: 11/20...  Training Step: 285...  Training loss: 0.2327...  30.6630 sec/batch\n",
      "Epoch: 11/20...  Training Step: 286...  Training loss: 0.1067...  28.8230 sec/batch\n",
      "Epoch: 11/20...  Training Step: 287...  Training loss: 0.0580...  29.5830 sec/batch\n",
      "Epoch: 11/20...  Training Step: 288...  Training loss: 0.1387...  32.8350 sec/batch\n",
      "Epoch: 11/20...  Training Step: 289...  Training loss: 0.0657...  29.7540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 290...  Training loss: 0.3017...  30.9970 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 291...  Training loss: 0.0981...  29.7360 sec/batch\n",
      "Epoch: 11/20...  Training Step: 292...  Training loss: 0.1637...  30.3590 sec/batch\n",
      "Epoch: 11/20...  Training Step: 293...  Training loss: 0.0818...  32.1750 sec/batch\n",
      "Epoch: 11/20...  Training Step: 294...  Training loss: 0.1506...  28.6240 sec/batch\n",
      "Epoch: 11/20...  Training Step: 295...  Training loss: 0.3447...  30.1740 sec/batch\n",
      "Epoch: 11/20...  Training Step: 296...  Training loss: 0.2481...  30.0380 sec/batch\n",
      "Epoch: 11/20...  Training Step: 297...  Training loss: 0.0920...  29.8530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 298...  Training loss: 0.3413...  32.2310 sec/batch\n",
      "Epoch: 12/20...  Training Step: 299...  Training loss: 0.2333...  29.6720 sec/batch\n",
      "Epoch: 12/20...  Training Step: 300...  Training loss: 0.1586...  31.5230 sec/batch\n",
      "Epoch: 12/20...  Training Step: 301...  Training loss: 0.0763...  32.9820 sec/batch\n",
      "Epoch: 12/20...  Training Step: 302...  Training loss: 0.1842...  33.2600 sec/batch\n",
      "Epoch: 12/20...  Training Step: 303...  Training loss: 0.1431...  30.3280 sec/batch\n",
      "Epoch: 12/20...  Training Step: 304...  Training loss: 0.0616...  30.4320 sec/batch\n",
      "Epoch: 12/20...  Training Step: 305...  Training loss: 0.2125...  29.1080 sec/batch\n",
      "Epoch: 12/20...  Training Step: 306...  Training loss: 0.1101...  29.5130 sec/batch\n",
      "Epoch: 12/20...  Training Step: 307...  Training loss: 0.1194...  31.6900 sec/batch\n",
      "Epoch: 12/20...  Training Step: 308...  Training loss: 0.2658...  32.6200 sec/batch\n",
      "Epoch: 12/20...  Training Step: 309...  Training loss: 0.1951...  31.1760 sec/batch\n",
      "Epoch: 12/20...  Training Step: 310...  Training loss: 0.0553...  28.5400 sec/batch\n",
      "Epoch: 12/20...  Training Step: 311...  Training loss: 0.1970...  33.8930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 312...  Training loss: 0.2071...  29.9050 sec/batch\n",
      "Epoch: 12/20...  Training Step: 313...  Training loss: 0.1431...  28.5020 sec/batch\n",
      "Epoch: 12/20...  Training Step: 314...  Training loss: 0.1519...  31.1880 sec/batch\n",
      "Epoch: 12/20...  Training Step: 315...  Training loss: 0.1323...  28.3160 sec/batch\n",
      "Epoch: 12/20...  Training Step: 316...  Training loss: 0.1408...  30.8540 sec/batch\n",
      "Epoch: 12/20...  Training Step: 317...  Training loss: 0.2772...  31.7160 sec/batch\n",
      "Epoch: 12/20...  Training Step: 318...  Training loss: 0.1565...  28.9300 sec/batch\n",
      "Epoch: 12/20...  Training Step: 319...  Training loss: 0.1348...  28.4350 sec/batch\n",
      "Epoch: 12/20...  Training Step: 320...  Training loss: 0.0664...  30.9290 sec/batch\n",
      "Epoch: 12/20...  Training Step: 321...  Training loss: 0.1553...  35.0960 sec/batch\n",
      "Epoch: 12/20...  Training Step: 322...  Training loss: 0.2117...  30.8610 sec/batch\n",
      "Epoch: 12/20...  Training Step: 323...  Training loss: 0.1878...  29.4730 sec/batch\n",
      "Epoch: 12/20...  Training Step: 324...  Training loss: 0.1817...  30.6380 sec/batch\n",
      "Epoch: 13/20...  Training Step: 325...  Training loss: 0.1727...  31.1840 sec/batch\n",
      "Epoch: 13/20...  Training Step: 326...  Training loss: 0.0773...  30.0890 sec/batch\n",
      "Epoch: 13/20...  Training Step: 327...  Training loss: 0.1937...  29.6950 sec/batch\n",
      "Epoch: 13/20...  Training Step: 328...  Training loss: 0.1728...  29.6530 sec/batch\n",
      "Epoch: 13/20...  Training Step: 329...  Training loss: 0.1114...  31.4770 sec/batch\n",
      "Epoch: 13/20...  Training Step: 330...  Training loss: 0.1789...  28.6750 sec/batch\n",
      "Epoch: 13/20...  Training Step: 331...  Training loss: 0.1781...  30.0460 sec/batch\n",
      "Epoch: 13/20...  Training Step: 332...  Training loss: 0.2401...  29.9340 sec/batch\n",
      "Epoch: 13/20...  Training Step: 333...  Training loss: 0.0677...  28.5430 sec/batch\n",
      "Epoch: 13/20...  Training Step: 334...  Training loss: 0.0959...  30.1630 sec/batch\n",
      "Epoch: 13/20...  Training Step: 335...  Training loss: 0.2681...  28.3220 sec/batch\n",
      "Epoch: 13/20...  Training Step: 336...  Training loss: 0.2109...  31.5090 sec/batch\n",
      "Epoch: 13/20...  Training Step: 337...  Training loss: 0.2260...  31.6880 sec/batch\n",
      "Epoch: 13/20...  Training Step: 338...  Training loss: 0.2190...  28.7060 sec/batch\n",
      "Epoch: 13/20...  Training Step: 339...  Training loss: 0.1613...  28.4650 sec/batch\n",
      "Epoch: 13/20...  Training Step: 340...  Training loss: 0.0852...  29.4350 sec/batch\n",
      "Epoch: 13/20...  Training Step: 341...  Training loss: 0.1209...  31.1960 sec/batch\n",
      "Epoch: 13/20...  Training Step: 342...  Training loss: 0.1258...  31.5520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 343...  Training loss: 0.1404...  30.2610 sec/batch\n",
      "Epoch: 13/20...  Training Step: 344...  Training loss: 0.1251...  30.7910 sec/batch\n",
      "Epoch: 13/20...  Training Step: 345...  Training loss: 0.0692...  29.7740 sec/batch\n",
      "Epoch: 13/20...  Training Step: 346...  Training loss: 0.2366...  29.8270 sec/batch\n",
      "Epoch: 13/20...  Training Step: 347...  Training loss: 0.1594...  31.3640 sec/batch\n",
      "Epoch: 13/20...  Training Step: 348...  Training loss: 0.0514...  29.6200 sec/batch\n",
      "Epoch: 13/20...  Training Step: 349...  Training loss: 0.1150...  31.4380 sec/batch\n",
      "Epoch: 13/20...  Training Step: 350...  Training loss: 0.1585...  29.9350 sec/batch\n",
      "Epoch: 13/20...  Training Step: 351...  Training loss: 0.1028...  31.4560 sec/batch\n",
      "Epoch: 14/20...  Training Step: 352...  Training loss: 0.1422...  31.7600 sec/batch\n",
      "Epoch: 14/20...  Training Step: 353...  Training loss: 0.1044...  28.2910 sec/batch\n",
      "Epoch: 14/20...  Training Step: 354...  Training loss: 0.2588...  30.1190 sec/batch\n",
      "Epoch: 14/20...  Training Step: 355...  Training loss: 0.2287...  28.4350 sec/batch\n",
      "Epoch: 14/20...  Training Step: 356...  Training loss: 0.1452...  29.8200 sec/batch\n",
      "Epoch: 14/20...  Training Step: 357...  Training loss: 0.1974...  31.2890 sec/batch\n",
      "Epoch: 14/20...  Training Step: 358...  Training loss: 0.2088...  30.2680 sec/batch\n",
      "Epoch: 14/20...  Training Step: 359...  Training loss: 0.2067...  29.7400 sec/batch\n",
      "Epoch: 14/20...  Training Step: 360...  Training loss: 0.1519...  29.6640 sec/batch\n",
      "Epoch: 14/20...  Training Step: 361...  Training loss: 0.0995...  30.8480 sec/batch\n",
      "Epoch: 14/20...  Training Step: 362...  Training loss: 0.3000...  28.8700 sec/batch\n",
      "Epoch: 14/20...  Training Step: 363...  Training loss: 0.1540...  30.0300 sec/batch\n",
      "Epoch: 14/20...  Training Step: 364...  Training loss: 0.2487...  29.7330 sec/batch\n",
      "Epoch: 14/20...  Training Step: 365...  Training loss: 0.1037...  29.9870 sec/batch\n",
      "Epoch: 14/20...  Training Step: 366...  Training loss: 0.2186...  31.7380 sec/batch\n",
      "Epoch: 14/20...  Training Step: 367...  Training loss: 0.1722...  31.2350 sec/batch\n",
      "Epoch: 14/20...  Training Step: 368...  Training loss: 0.1226...  29.6360 sec/batch\n",
      "Epoch: 14/20...  Training Step: 369...  Training loss: 0.1800...  28.7410 sec/batch\n",
      "Epoch: 14/20...  Training Step: 370...  Training loss: 0.1518...  28.7390 sec/batch\n",
      "Epoch: 14/20...  Training Step: 371...  Training loss: 0.1913...  32.0130 sec/batch\n",
      "Epoch: 14/20...  Training Step: 372...  Training loss: 0.0777...  28.6300 sec/batch\n",
      "Epoch: 14/20...  Training Step: 373...  Training loss: 0.3040...  28.5350 sec/batch\n",
      "Epoch: 14/20...  Training Step: 374...  Training loss: 0.1756...  30.9330 sec/batch\n",
      "Epoch: 14/20...  Training Step: 375...  Training loss: 0.2322...  31.7300 sec/batch\n",
      "Epoch: 14/20...  Training Step: 376...  Training loss: 0.1624...  33.3650 sec/batch\n",
      "Epoch: 14/20...  Training Step: 377...  Training loss: 0.2928...  32.6840 sec/batch\n",
      "Epoch: 14/20...  Training Step: 378...  Training loss: 0.1523...  31.3660 sec/batch\n",
      "Epoch: 15/20...  Training Step: 379...  Training loss: 0.1960...  30.9470 sec/batch\n",
      "Epoch: 15/20...  Training Step: 380...  Training loss: 0.0900...  29.6790 sec/batch\n",
      "Epoch: 15/20...  Training Step: 381...  Training loss: 0.2754...  30.1770 sec/batch\n",
      "Epoch: 15/20...  Training Step: 382...  Training loss: 0.2011...  29.4620 sec/batch\n",
      "Epoch: 15/20...  Training Step: 383...  Training loss: 0.2478...  29.8190 sec/batch\n",
      "Epoch: 15/20...  Training Step: 384...  Training loss: 0.2483...  30.9660 sec/batch\n",
      "Epoch: 15/20...  Training Step: 385...  Training loss: 0.1636...  28.7250 sec/batch\n",
      "Epoch: 15/20...  Training Step: 386...  Training loss: 0.1450...  32.6240 sec/batch\n",
      "Epoch: 15/20...  Training Step: 387...  Training loss: 0.1086...  31.5540 sec/batch\n",
      "Epoch: 15/20...  Training Step: 388...  Training loss: 0.0710...  32.0280 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 389...  Training loss: 0.3118...  31.2150 sec/batch\n",
      "Epoch: 15/20...  Training Step: 390...  Training loss: 0.2105...  31.2360 sec/batch\n",
      "Epoch: 15/20...  Training Step: 391...  Training loss: 0.0698...  32.9170 sec/batch\n",
      "Epoch: 15/20...  Training Step: 392...  Training loss: 0.1321...  31.7380 sec/batch\n",
      "Epoch: 15/20...  Training Step: 393...  Training loss: 0.1943...  28.5750 sec/batch\n",
      "Epoch: 15/20...  Training Step: 394...  Training loss: 0.0973...  28.8480 sec/batch\n",
      "Epoch: 15/20...  Training Step: 395...  Training loss: 0.1511...  30.1600 sec/batch\n",
      "Epoch: 15/20...  Training Step: 396...  Training loss: 0.1281...  31.7720 sec/batch\n",
      "Epoch: 15/20...  Training Step: 397...  Training loss: 0.1475...  28.8610 sec/batch\n",
      "Epoch: 15/20...  Training Step: 398...  Training loss: 0.2302...  34.9440 sec/batch\n",
      "Epoch: 15/20...  Training Step: 399...  Training loss: 0.0503...  33.1860 sec/batch\n",
      "Epoch: 15/20...  Training Step: 400...  Training loss: 0.1383...  28.7800 sec/batch\n",
      "Epoch: 15/20...  Training Step: 401...  Training loss: 0.1619...  30.8670 sec/batch\n",
      "Epoch: 15/20...  Training Step: 402...  Training loss: 0.0671...  29.0280 sec/batch\n",
      "Epoch: 15/20...  Training Step: 403...  Training loss: 0.1164...  29.2430 sec/batch\n",
      "Epoch: 15/20...  Training Step: 404...  Training loss: 0.3521...  30.5260 sec/batch\n",
      "Epoch: 15/20...  Training Step: 405...  Training loss: 0.1352...  29.5200 sec/batch\n",
      "Epoch: 16/20...  Training Step: 406...  Training loss: 0.1589...  32.7630 sec/batch\n",
      "Epoch: 16/20...  Training Step: 407...  Training loss: 0.2310...  35.0780 sec/batch\n",
      "Epoch: 16/20...  Training Step: 408...  Training loss: 0.2904...  30.5100 sec/batch\n",
      "Epoch: 16/20...  Training Step: 409...  Training loss: 0.1289...  32.4470 sec/batch\n",
      "Epoch: 16/20...  Training Step: 410...  Training loss: 0.1733...  31.8180 sec/batch\n",
      "Epoch: 16/20...  Training Step: 411...  Training loss: 0.2087...  41.7090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 412...  Training loss: 0.1270...  41.1240 sec/batch\n",
      "Epoch: 16/20...  Training Step: 413...  Training loss: 0.0915...  37.4890 sec/batch\n",
      "Epoch: 16/20...  Training Step: 414...  Training loss: 0.1014...  41.1010 sec/batch\n",
      "Epoch: 16/20...  Training Step: 415...  Training loss: 0.1027...  38.6010 sec/batch\n",
      "Epoch: 16/20...  Training Step: 416...  Training loss: 0.2478...  37.1240 sec/batch\n",
      "Epoch: 16/20...  Training Step: 417...  Training loss: 0.2710...  37.2940 sec/batch\n",
      "Epoch: 16/20...  Training Step: 418...  Training loss: 0.2714...  38.4600 sec/batch\n",
      "Epoch: 16/20...  Training Step: 419...  Training loss: 0.1820...  37.0180 sec/batch\n",
      "Epoch: 16/20...  Training Step: 420...  Training loss: 0.2294...  35.6950 sec/batch\n",
      "Epoch: 16/20...  Training Step: 421...  Training loss: 0.1677...  30.3770 sec/batch\n",
      "Epoch: 16/20...  Training Step: 422...  Training loss: 0.1138...  31.5910 sec/batch\n",
      "Epoch: 16/20...  Training Step: 423...  Training loss: 0.1507...  32.1890 sec/batch\n",
      "Epoch: 16/20...  Training Step: 424...  Training loss: 0.1043...  36.6630 sec/batch\n",
      "Epoch: 16/20...  Training Step: 425...  Training loss: 0.1687...  36.4090 sec/batch\n",
      "Epoch: 16/20...  Training Step: 426...  Training loss: 0.0392...  41.2710 sec/batch\n",
      "Epoch: 16/20...  Training Step: 427...  Training loss: 0.2449...  34.8510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 428...  Training loss: 0.0729...  30.1060 sec/batch\n",
      "Epoch: 16/20...  Training Step: 429...  Training loss: 0.0526...  28.9100 sec/batch\n",
      "Epoch: 16/20...  Training Step: 430...  Training loss: 0.1878...  29.7830 sec/batch\n",
      "Epoch: 16/20...  Training Step: 431...  Training loss: 0.2137...  32.4140 sec/batch\n",
      "Epoch: 16/20...  Training Step: 432...  Training loss: 0.2213...  31.9410 sec/batch\n",
      "Epoch: 17/20...  Training Step: 433...  Training loss: 0.2788...  36.5820 sec/batch\n",
      "Epoch: 17/20...  Training Step: 434...  Training loss: 0.1291...  38.1600 sec/batch\n",
      "Epoch: 17/20...  Training Step: 435...  Training loss: 0.1825...  34.2760 sec/batch\n",
      "Epoch: 17/20...  Training Step: 436...  Training loss: 0.1356...  34.9710 sec/batch\n",
      "Epoch: 17/20...  Training Step: 437...  Training loss: 0.1819...  38.6800 sec/batch\n",
      "Epoch: 17/20...  Training Step: 438...  Training loss: 0.1517...  40.1980 sec/batch\n",
      "Epoch: 17/20...  Training Step: 439...  Training loss: 0.1116...  43.1870 sec/batch\n",
      "Epoch: 17/20...  Training Step: 440...  Training loss: 0.1605...  36.4650 sec/batch\n",
      "Epoch: 17/20...  Training Step: 441...  Training loss: 0.1074...  31.6890 sec/batch\n",
      "Epoch: 17/20...  Training Step: 442...  Training loss: 0.1112...  35.5370 sec/batch\n",
      "Epoch: 17/20...  Training Step: 443...  Training loss: 0.3164...  41.1400 sec/batch\n",
      "Epoch: 17/20...  Training Step: 444...  Training loss: 0.1739...  31.5300 sec/batch\n",
      "Epoch: 17/20...  Training Step: 445...  Training loss: 0.1135...  33.6420 sec/batch\n",
      "Epoch: 17/20...  Training Step: 446...  Training loss: 0.1304...  37.2680 sec/batch\n",
      "Epoch: 17/20...  Training Step: 447...  Training loss: 0.2547...  39.8010 sec/batch\n",
      "Epoch: 17/20...  Training Step: 448...  Training loss: 0.2327...  39.3500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 449...  Training loss: 0.1108...  36.1770 sec/batch\n",
      "Epoch: 17/20...  Training Step: 450...  Training loss: 0.1600...  40.6480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 451...  Training loss: 0.1247...  41.2530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 452...  Training loss: 0.2834...  45.2860 sec/batch\n",
      "Epoch: 17/20...  Training Step: 453...  Training loss: 0.1101...  50.1630 sec/batch\n",
      "Epoch: 17/20...  Training Step: 454...  Training loss: 0.1481...  53.4500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 455...  Training loss: 0.1295...  50.6600 sec/batch\n",
      "Epoch: 17/20...  Training Step: 456...  Training loss: 0.0986...  40.7510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 457...  Training loss: 0.2158...  44.9360 sec/batch\n",
      "Epoch: 17/20...  Training Step: 458...  Training loss: 0.2147...  40.5380 sec/batch\n",
      "Epoch: 17/20...  Training Step: 459...  Training loss: 0.0702...  38.0690 sec/batch\n",
      "Epoch: 18/20...  Training Step: 460...  Training loss: 0.1348...  39.2710 sec/batch\n",
      "Epoch: 18/20...  Training Step: 461...  Training loss: 0.2334...  43.3770 sec/batch\n",
      "Epoch: 18/20...  Training Step: 462...  Training loss: 0.1737...  41.1080 sec/batch\n",
      "Epoch: 18/20...  Training Step: 463...  Training loss: 0.2310...  38.0150 sec/batch\n",
      "Epoch: 18/20...  Training Step: 464...  Training loss: 0.1415...  40.3980 sec/batch\n",
      "Epoch: 18/20...  Training Step: 465...  Training loss: 0.0908...  39.0900 sec/batch\n",
      "Epoch: 18/20...  Training Step: 466...  Training loss: 0.0931...  38.8870 sec/batch\n",
      "Epoch: 18/20...  Training Step: 467...  Training loss: 0.1900...  38.8020 sec/batch\n",
      "Epoch: 18/20...  Training Step: 468...  Training loss: 0.1218...  33.2660 sec/batch\n",
      "Epoch: 18/20...  Training Step: 469...  Training loss: 0.1284...  37.5300 sec/batch\n",
      "Epoch: 18/20...  Training Step: 470...  Training loss: 0.2326...  37.4080 sec/batch\n",
      "Epoch: 18/20...  Training Step: 471...  Training loss: 0.2004...  36.8610 sec/batch\n",
      "Epoch: 18/20...  Training Step: 472...  Training loss: 0.1403...  35.5970 sec/batch\n",
      "Epoch: 18/20...  Training Step: 473...  Training loss: 0.1611...  42.4840 sec/batch\n",
      "Epoch: 18/20...  Training Step: 474...  Training loss: 0.2385...  39.1280 sec/batch\n",
      "Epoch: 18/20...  Training Step: 475...  Training loss: 0.0918...  40.6500 sec/batch\n",
      "Epoch: 18/20...  Training Step: 476...  Training loss: 0.0895...  40.0460 sec/batch\n",
      "Epoch: 18/20...  Training Step: 477...  Training loss: 0.1203...  46.7450 sec/batch\n",
      "Epoch: 18/20...  Training Step: 478...  Training loss: 0.1083...  34.6410 sec/batch\n",
      "Epoch: 18/20...  Training Step: 479...  Training loss: 0.2774...  36.5860 sec/batch\n",
      "Epoch: 18/20...  Training Step: 480...  Training loss: 0.1926...  35.3050 sec/batch\n",
      "Epoch: 18/20...  Training Step: 481...  Training loss: 0.2338...  48.1310 sec/batch\n",
      "Epoch: 18/20...  Training Step: 482...  Training loss: 0.2054...  42.1100 sec/batch\n",
      "Epoch: 18/20...  Training Step: 483...  Training loss: 0.1299...  41.6620 sec/batch\n",
      "Epoch: 18/20...  Training Step: 484...  Training loss: 0.2169...  42.3980 sec/batch\n",
      "Epoch: 18/20...  Training Step: 485...  Training loss: 0.1491...  43.1840 sec/batch\n",
      "Epoch: 18/20...  Training Step: 486...  Training loss: 0.1221...  40.7000 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 487...  Training loss: 0.2109...  41.1070 sec/batch\n",
      "Epoch: 19/20...  Training Step: 488...  Training loss: 0.1734...  44.2260 sec/batch\n",
      "Epoch: 19/20...  Training Step: 489...  Training loss: 0.2016...  38.7360 sec/batch\n",
      "Epoch: 19/20...  Training Step: 490...  Training loss: 0.1963...  42.7790 sec/batch\n",
      "Epoch: 19/20...  Training Step: 491...  Training loss: 0.1934...  41.8740 sec/batch\n",
      "Epoch: 19/20...  Training Step: 492...  Training loss: 0.0906...  42.5860 sec/batch\n",
      "Epoch: 19/20...  Training Step: 493...  Training loss: 0.1122...  38.9960 sec/batch\n",
      "Epoch: 19/20...  Training Step: 494...  Training loss: 0.0986...  43.3880 sec/batch\n",
      "Epoch: 19/20...  Training Step: 495...  Training loss: 0.1047...  44.4160 sec/batch\n",
      "Epoch: 19/20...  Training Step: 496...  Training loss: 0.1588...  41.1450 sec/batch\n",
      "Epoch: 19/20...  Training Step: 497...  Training loss: 0.2256...  38.8330 sec/batch\n",
      "Epoch: 19/20...  Training Step: 498...  Training loss: 0.0801...  38.2160 sec/batch\n",
      "Epoch: 19/20...  Training Step: 499...  Training loss: 0.0778...  40.4930 sec/batch\n",
      "Epoch: 19/20...  Training Step: 500...  Training loss: 0.2161...  39.7890 sec/batch\n",
      "Epoch: 19/20...  Training Step: 501...  Training loss: 0.1436...  42.5320 sec/batch\n",
      "Epoch: 19/20...  Training Step: 502...  Training loss: 0.1010...  43.4830 sec/batch\n",
      "Epoch: 19/20...  Training Step: 503...  Training loss: 0.1528...  40.9580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 504...  Training loss: 0.1075...  41.5620 sec/batch\n",
      "Epoch: 19/20...  Training Step: 505...  Training loss: 0.1781...  33.1820 sec/batch\n",
      "Epoch: 19/20...  Training Step: 506...  Training loss: 0.2451...  32.1330 sec/batch\n",
      "Epoch: 19/20...  Training Step: 507...  Training loss: 0.1321...  43.4680 sec/batch\n",
      "Epoch: 19/20...  Training Step: 508...  Training loss: 0.1544...  50.2250 sec/batch\n",
      "Epoch: 19/20...  Training Step: 509...  Training loss: 0.1305...  49.1180 sec/batch\n",
      "Epoch: 19/20...  Training Step: 510...  Training loss: 0.1242...  40.4340 sec/batch\n",
      "Epoch: 19/20...  Training Step: 511...  Training loss: 0.1985...  45.0230 sec/batch\n",
      "Epoch: 19/20...  Training Step: 512...  Training loss: 0.2600...  39.2930 sec/batch\n",
      "Epoch: 19/20...  Training Step: 513...  Training loss: 0.1991...  45.4540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 514...  Training loss: 0.1825...  41.2950 sec/batch\n",
      "Epoch: 20/20...  Training Step: 515...  Training loss: 0.2333...  40.8890 sec/batch\n",
      "Epoch: 20/20...  Training Step: 516...  Training loss: 0.0941...  39.2510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 517...  Training loss: 0.1093...  42.1370 sec/batch\n",
      "Epoch: 20/20...  Training Step: 518...  Training loss: 0.1673...  39.8900 sec/batch\n",
      "Epoch: 20/20...  Training Step: 519...  Training loss: 0.1236...  39.4640 sec/batch\n",
      "Epoch: 20/20...  Training Step: 520...  Training loss: 0.1812...  41.9460 sec/batch\n",
      "Epoch: 20/20...  Training Step: 521...  Training loss: 0.1332...  41.4650 sec/batch\n",
      "Epoch: 20/20...  Training Step: 522...  Training loss: 0.1519...  38.8230 sec/batch\n",
      "Epoch: 20/20...  Training Step: 523...  Training loss: 0.1509...  46.0750 sec/batch\n",
      "Epoch: 20/20...  Training Step: 524...  Training loss: 0.1713...  45.6780 sec/batch\n",
      "Epoch: 20/20...  Training Step: 525...  Training loss: 0.0708...  42.1780 sec/batch\n",
      "Epoch: 20/20...  Training Step: 526...  Training loss: 0.0780...  41.7740 sec/batch\n",
      "Epoch: 20/20...  Training Step: 527...  Training loss: 0.0929...  40.5500 sec/batch\n",
      "Epoch: 20/20...  Training Step: 528...  Training loss: 0.2325...  43.8070 sec/batch\n",
      "Epoch: 20/20...  Training Step: 529...  Training loss: 0.1677...  40.9760 sec/batch\n",
      "Epoch: 20/20...  Training Step: 530...  Training loss: 0.1754...  40.3840 sec/batch\n",
      "Epoch: 20/20...  Training Step: 531...  Training loss: 0.1374...  41.7260 sec/batch\n",
      "Epoch: 20/20...  Training Step: 532...  Training loss: 0.0816...  37.4570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 533...  Training loss: 0.2134...  41.7790 sec/batch\n",
      "Epoch: 20/20...  Training Step: 534...  Training loss: 0.1188...  40.1550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 535...  Training loss: 0.2364...  42.0120 sec/batch\n",
      "Epoch: 20/20...  Training Step: 536...  Training loss: 0.0886...  41.1390 sec/batch\n",
      "Epoch: 20/20...  Training Step: 537...  Training loss: 0.0697...  41.9070 sec/batch\n",
      "Epoch: 20/20...  Training Step: 538...  Training loss: 0.1847...  38.4170 sec/batch\n",
      "Epoch: 20/20...  Training Step: 539...  Training loss: 0.3202...  31.0410 sec/batch\n",
      "Epoch: 20/20...  Training Step: 540...  Training loss: 0.1258...  31.4860 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "save_every_n = 200\n",
    "\n",
    "model = YelpNetwork(lstm_size,batch_size,num_layers,dictionary)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for e in range (epoch):\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0.0\n",
    "        for x,y in create_batch(feature,labels,batch_size):\n",
    "        #while True:\n",
    "            #x,y=next(create_batches(encoded, batch_size, num_steps))\n",
    "            counter +=1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.target: y,\n",
    "                    model.keep_proba: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            \n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer\n",
    "                                                 ], \n",
    "                                                 feed_dict=feed)\n",
    "            loss +=batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epoch),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  #'Training state: {:.4f}... '.format(new_state),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    #print ('Out : ',out) \n",
    "    #print ('Logits : ',logits)\n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature,test_labels=create_data('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-21-c57825aac3f0>:57: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from checkpoints\\i540_l512.ckpt\n",
      "Test Accuracy:  0.8038461\n"
     ]
    }
   ],
   "source": [
    "test_accu=[]\n",
    "out_put=[]\n",
    "model = YelpNetwork(lstm_size,batch_size,num_layers,dictionary)\n",
    "saver=tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('checkpoints'))\n",
    "    #saver.restore(sess,'checkpoints\\\\i10_l512.ckpt')\n",
    "    test_state=sess.run(model.initial_state)\n",
    "    for x,y in create_batch(test_feature,test_labels,batch_size,test=True):\n",
    "        feed = {model.inputs: x,\n",
    "                model.target: y,\n",
    "                model.keep_proba: 1,\n",
    "                model.initial_state: test_state}\n",
    "        accu,test_state,out=sess.run([model.accuracy,model.final_state,model.out],feed_dict=feed)\n",
    "        test_accu.append(accu)\n",
    "        out_put.extend(out)\n",
    "        \n",
    "    print('Test Accuracy: ',np.mean(test_accu))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
